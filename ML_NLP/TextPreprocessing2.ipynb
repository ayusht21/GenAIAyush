{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69732cd1",
   "metadata": {},
   "source": [
    "### Refer finalNLP pdf\n",
    "### Conversion of words to Vectors\n",
    "\n",
    "1) One Hot Encoding technique - <br> \n",
    "suppose you have 3 docs <br> \n",
    "D1) The food is good <br> \n",
    "D2) The food is bad <br> \n",
    "D3) Pizza is Amazing <br> \n",
    "\n",
    "now it will check the vocabulary size which is 7 [The, food, is, good, bad, Pizza, Amazing]<br> \n",
    "now for each of the 3 doc vector will be created as (No of words in doc x Vocabulary size)() eg d1 - 4 x 7 <br> \n",
    "[The, food, is, good, bad, Pizza, Amazing]  <br> \n",
    "D1 [ [1,0,0,0,0,0,0],  <br> \n",
    "    [0,1,0,0,0,0,0], <br> \n",
    "    [0,0,1,0,0,0,0], <br> \n",
    "    [0,0,0,1,0,0,0] <br> \n",
    "] - this shows the position of each vocabulary words in document<br> \n",
    "simillarly for other also <br> \n",
    "This technique is not used much due to some limitations<br>\n",
    "\n",
    "### Advantages of OHE\n",
    "1. Easy to implement with python - sklearn - onehotencoder or pd.get_dummies() <br>\n",
    "\n",
    "### Disadvantagess\n",
    "1. Sparse matrix - overfitting (too much numbers) <br>\n",
    "2. ML Algo needs fixed text size - no fixed text size (Doc 1 and 2 - 4 x 7 and Doc 3 - 3 x 7)<br>\n",
    "3. No semantic meaning is captured - can't understand what each word is important (each word is equally important)<br>\n",
    "4. Out of vocabulary - if new word comes then it won't work <br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f62cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b804536",
   "metadata": {},
   "source": [
    "### Bag of Words (Bow)\n",
    "Convert to lower case then use stopwords then below process <br>\n",
    "It will also find the vocubulary words and the frequency of each word in decreasing order of frequency <br>\n",
    "based on each sentence then vector will be created after applying stopwords <br>\n",
    "Refer the finalNLP pdf for better understanding <br>\n",
    "Binary BOW (0,1) but normal Bow(count based on frequency)<br>\n",
    "\n",
    "Advantages<br>\n",
    "1. Easy to implement <br>\n",
    "2. Fixed size input - helps ML algorithm<br>\n",
    "\n",
    "Disadvantages<br>\n",
    "1. Sparce matrix is there - overfitting<br>\n",
    "2. Ordering of the words is changed <br>\n",
    "3. Out of vocabulary still persists here as well <br>\n",
    "4. Semantic meaning still not getting captured - important word not able to get  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54551e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dabc7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>tell me anything about you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>tfor fear of fainting with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Thanks for your subscription</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                           message\n",
       "0   ham       tell me anything about you.\n",
       "1   ham       tfor fear of fainting with \n",
       "2  spam  Thanks for your subscription    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "messages = pd.read_csv('Spamtext.txt', sep='!', names=['label', 'message'])\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b62e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Data cleaning and pre processing \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f90e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'me', 'anything', 'about', 'you']\n",
      "['tfor', 'fear', 'of', 'fainting', 'with']\n",
      "['thanks', 'for', 'your', 'subscription']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    print(review)\n",
    "    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88746c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create bag of words\n",
    "### look into the countvectorizer class \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "### for binary bag of words enable binary = true \n",
    "cv = CountVectorizer(max_features=2500, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89fd2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f4e2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b72270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388506d6",
   "metadata": {},
   "source": [
    "### N grams\n",
    "Refer finalNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a338b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'me', 'anything', 'about', 'you']\n",
      "['tfor', 'fear', 'of', 'fainting', 'with']\n",
      "['thanks', 'for', 'your', 'subscription']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "messages = pd.read_csv('Spamtext.txt', sep='!', names=['label', 'message'])\n",
    "messages\n",
    "### Data cleaning and pre processing \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    print(review)\n",
    "    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "### create bag of words\n",
    "### look into the countvectorizer class  using N- gram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "### for binary bag of words enable binary = true \n",
    "cv = CountVectorizer(max_features=2500, binary=True, ngram_range=(1,2))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e860b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e7f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473116b0",
   "metadata": {},
   "source": [
    "### TF-IDF (Term frequency - Inverse document frequency)\n",
    "Refer finalNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757ab1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
