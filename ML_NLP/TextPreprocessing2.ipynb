{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69732cd1",
   "metadata": {},
   "source": [
    "### Refer finalNLP pdf\n",
    "### Conversion of words to Vectors\n",
    "\n",
    "1) One Hot Encoding technique - <br> \n",
    "suppose you have 3 docs <br> \n",
    "D1) The food is good <br> \n",
    "D2) The food is bad <br> \n",
    "D3) Pizza is Amazing <br> \n",
    "\n",
    "now it will check the vocabulary size which is 7 [The, food, is, good, bad, Pizza, Amazing]<br> \n",
    "now for each of the 3 doc vector will be created as (No of words in doc x Vocabulary size)() eg d1 - 4 x 7 <br> \n",
    "[The, food, is, good, bad, Pizza, Amazing]  <br> \n",
    "D1 [ [1,0,0,0,0,0,0],  <br> \n",
    "    [0,1,0,0,0,0,0], <br> \n",
    "    [0,0,1,0,0,0,0], <br> \n",
    "    [0,0,0,1,0,0,0] <br> \n",
    "] - this shows the position of each vocabulary words in document<br> \n",
    "simillarly for other also <br> \n",
    "This technique is not used much due to some limitations<br>\n",
    "\n",
    "### Advantages of OHE\n",
    "1. Easy to implement with python - sklearn - onehotencoder or pd.get_dummies() <br>\n",
    "\n",
    "### Disadvantagess\n",
    "1. Sparse matrix - overfitting (too much numbers) <br>\n",
    "2. ML Algo needs fixed text size - no fixed text size (Doc 1 and 2 - 4 x 7 and Doc 3 - 3 x 7)<br>\n",
    "3. No semantic meaning is captured - can't understand what each word is important (each word is equally important)<br>\n",
    "4. Out of vocabulary - if new word comes then it won't work <br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f62cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b804536",
   "metadata": {},
   "source": [
    "### Bag of Words (Bow)\n",
    "Convert to lower case then use stopwords then below process <br>\n",
    "It will also find the vocubulary words and the frequency of each word in decreasing order of frequency <br>\n",
    "based on each sentence then vector will be created after applying stopwords <br>\n",
    "Refer the finalNLP pdf for better understanding <br>\n",
    "Binary BOW (0,1) but normal Bow(count based on frequency)<br>\n",
    "\n",
    "Advantages<br>\n",
    "1. Easy to implement <br>\n",
    "2. Fixed size input - helps ML algorithm<br>\n",
    "\n",
    "Disadvantages<br>\n",
    "1. Sparce matrix is there - overfitting<br>\n",
    "2. Ordering of the words is changed <br>\n",
    "3. Out of vocabulary still persists here as well <br>\n",
    "4. Semantic meaning still not getting captured - important word not able to get  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54551e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dabc7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>tell me anything about you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>tfor fear of fainting with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Thanks for your subscription</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                           message\n",
       "0   ham       tell me anything about you.\n",
       "1   ham       tfor fear of fainting with \n",
       "2  spam  Thanks for your subscription    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "messages = pd.read_csv('Spamtext.txt', sep='!', names=['label', 'message'])\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b62e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Data cleaning and pre processing \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f90e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'me', 'anything', 'about', 'you']\n",
      "['tfor', 'fear', 'of', 'fainting', 'with']\n",
      "['thanks', 'for', 'your', 'subscription']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    print(review)\n",
    "    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b88746c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create bag of words\n",
    "### look into the countvectorizer class \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "### for binary bag of words enable binary = true \n",
    "cv = CountVectorizer(max_features=2500, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89fd2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68f4e2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b72270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388506d6",
   "metadata": {},
   "source": [
    "### N grams\n",
    "Refer finalNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a338b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'me', 'anything', 'about', 'you']\n",
      "['tfor', 'fear', 'of', 'fainting', 'with']\n",
      "['thanks', 'for', 'your', 'subscription']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "messages = pd.read_csv('Spamtext.txt', sep='!', names=['label', 'message'])\n",
    "messages\n",
    "### Data cleaning and pre processing \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    print(review)\n",
    "    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "### create bag of words\n",
    "### look into the countvectorizer class  using N- gram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "### for binary bag of words enable binary = true \n",
    "cv = CountVectorizer(max_features=2500, binary=True, ngram_range=(1,2))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e860b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32e7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473116b0",
   "metadata": {},
   "source": [
    "### TF-IDF (Term frequency - Inverse document frequency)\n",
    "Refer finalNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9013c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell anything', 'fear fainting', 'thanks subscription']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "messages = pd.read_csv('Spamtext.txt', sep='!', names=['label', 'message'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlemma = WordNetLemmatizer()\n",
    "\n",
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordlemma.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ff100",
   "metadata": {},
   "source": [
    "Use TfidfVectorizer class \n",
    "import sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "Simillar to Bow\n",
    "max features = 100 gives top 100 repeated words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5dff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tell anything': np.int64(1),\n",
       " 'fear fainting': np.int64(0),\n",
       " 'thanks subscription': np.int64(2)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdidf = TfidfVectorizer(max_features=100, ngram_range=(2,2))\n",
    "x = tdidf.fit_transform(corpus).toarray()\n",
    "tdidf.vocabulary_\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11cf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2507c344",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Refer finalNLP pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a7a5b",
   "metadata": {},
   "source": [
    "### WordtoVec\n",
    "Refer NLP\n",
    "each words - converted to vector based on features - (feature representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980fc8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c4456",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "### using a pretrained model to do our work \n",
    "### word2vec-google-news-300 check in huggingface api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5de4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
       "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
       "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
       "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
       "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
       "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
       "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
       "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
       "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
       "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
       "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
       "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
       "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
       "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
       "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
       "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
       "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
       "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
       "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
       "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
       "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
       "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
       "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
       "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
       "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
       "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
       "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
       "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
       "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
       "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
       "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
       "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
       "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
       "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
       "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
       "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
       "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
       "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
       "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
       "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
       "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
       "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
       "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
       "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
       "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
       "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
       "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
       "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
       "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
       "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
       "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
       "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
       "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
       "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
       "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
       "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
       "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
       "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
       "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
       "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
       "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
       "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
       "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
       "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
       "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
       "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
       "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
       "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
       "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
       "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
       "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
       "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
       "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
       "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
       "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "### to get vector of 300 dimension as mentioned by google \n",
    "vec_king = wv['king']\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1c503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cricketing', 0.8372225761413574),\n",
       " ('cricketers', 0.8165745735168457),\n",
       " ('Test_cricket', 0.8094819188117981),\n",
       " ('Twenty##_cricket', 0.8068488240242004),\n",
       " ('Twenty##', 0.7624265551567078),\n",
       " ('Cricket', 0.75413978099823),\n",
       " ('cricketer', 0.7372578382492065),\n",
       " ('twenty##', 0.7316356897354126),\n",
       " ('T##_cricket', 0.7304614186286926),\n",
       " ('West_Indies_cricket', 0.6987985968589783)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b908be33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glad', 0.7408890724182129),\n",
       " ('pleased', 0.6632170677185059),\n",
       " ('ecstatic', 0.6626912355422974),\n",
       " ('overjoyed', 0.6599286794662476),\n",
       " ('thrilled', 0.6514049172401428),\n",
       " ('satisfied', 0.6437949538230896),\n",
       " ('proud', 0.636042058467865),\n",
       " ('delighted', 0.627237856388092),\n",
       " ('disappointed', 0.6269949674606323),\n",
       " ('excited', 0.6247665286064148)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd2a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.53541523)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('hockey','sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc1deae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613663792610168),\n",
       " ('sultan', 0.5376775860786438),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = wv['king']-wv['man']+wv['woman']\n",
    "wv.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5095a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
