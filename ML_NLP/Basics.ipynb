{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459624a8",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "corpus = paragraph\n",
    "\n",
    "document = sentences\n",
    "\n",
    "vocabulary = unique words\n",
    "\n",
    "words\n",
    "\n",
    "whole sentence or paragraph will be converted to tokens. First it will convert paragraph to multiple sentences by parsing across . or any other special characters. Now these sentences will be converted into words (this is also tokenization).\n",
    "\n",
    "It will count the number of unique words - Vocabulary part\n",
    "\n",
    "tokenization - conversion of corpus to document or conversion of document to vocabulary etc\n",
    "\n",
    "NLTK library and spaCy library we use for NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe4c06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f149fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome, to Ayush Tidke learning NLP tutorial.\n",
      "Please do support me in learning and buidling different things\n"
     ]
    }
   ],
   "source": [
    "Corpus = \"\"\"Hello welcome, to Ayush Tidke learning NLP tutorial.\n",
    "Please do support me in learning and buidling different things\"\"\"\n",
    "print(Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4082e",
   "metadata": {},
   "source": [
    "sent_tokenize - sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325ab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization\n",
    "## Sentence --> paragraphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "documents = sent_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f3adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome, to Ayush Tidke learning NLP tutorial.\n",
      "Please do support me in learning and buidling different things\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292848b",
   "metadata": {},
   "source": [
    "Paragraph -- words\n",
    "sentence to words tokenization\n",
    "word_tokenize - gives list of every word in the paragraph or sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750f0e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ayush',\n",
       " 'Tidke',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'support',\n",
       " 'me',\n",
       " 'in',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'buidling',\n",
       " 'different',\n",
       " 'things']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032d5b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Ayush',\n",
       " 'Tidke',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'support',\n",
       " 'me',\n",
       " 'in',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'buidling',\n",
       " 'different',\n",
       " 'things']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### is considers all the punctuations as a character as well \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7c7e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H e l l o   w e l c o m e,   t o   A y u s h   T i d k e   l e a r n i n g   N L P   t u t o r i a l . \\n P l e a s e   d o   s u p p o r t   m e   i n   l e a r n i n g   a n d   b u i d l i n g   d i f f e r e n t   t h i n g s'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Treebankwordtokenizer doesnot consider fullstop as a single character \n",
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "tokenizer = TreebankWordDetokenizer()\n",
    "tokenizer.tokenize(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961367e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5b4f365",
   "metadata": {},
   "source": [
    "### Text Preprocessing Stemming \n",
    "Stemming is the process of reducing a word to its word stem that affixes or suffixes and prefixes or to the roots of words known as a lemma. Stemming is imp in NLU (Natural language understanding) and NLP \n",
    "eg. Classify the comments of product is a positive or negative review\n",
    "\n",
    "Disadvantage : for some of the words you may not get the exact meaning. the form of the specific word may change\n",
    "\n",
    "There are various ways to do stemming \n",
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7266ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[\"writing\",\"write\",\"programming\",\"Program\",\"history\",\"finally\",\"finalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ab1a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ---> write\n",
      "write ---> write\n",
      "programming ---> program\n",
      "Program ---> program\n",
      "history ---> histori\n",
      "finally ---> final\n",
      "finalize ---> final\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\" ---> \" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b118dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f6a43",
   "metadata": {},
   "source": [
    "### Regex Stemmer class\n",
    "using this we can implement Regular expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447a22c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### according to the regex that you send it stems and the position of $ sign indicates where to remove (end, start etc)\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4e5e7",
   "metadata": {},
   "source": [
    "### Snowball Stemmer \n",
    "Better than PorterStemmer (more accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aede5966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n",
      "write\n",
      "program\n",
      "program\n",
      "histori\n",
      "final\n",
      "final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5f812",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is like stemming. The output that we get after lemmatization is called 'lemma', which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around wordnet corpus. This class uses morphy() function to the WordNet Corpus Reader class to find a lemma.\n",
    "\n",
    "Pos tag\n",
    "Noun - n (default) |\n",
    "verb - v |\n",
    "Adjective - a |\n",
    "adverb - r\n",
    "\n",
    "### When to use\n",
    "Q&A | Chatbots | Text summarization | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32b904cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pos tag is must\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27a240a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing\n",
      "write\n",
      "programming\n",
      "Program\n",
      "history\n",
      "finally\n",
      "finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word,pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce0b60",
   "metadata": {},
   "source": [
    "### Text Processing Stopwords\n",
    "it removes the unnecessary words from the paragraph \n",
    "you can also create your own stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270472ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270b6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Ayush_Tidke\\Studies\\Coding_World\\GenAIUdemy\\venv\\Lib\\site-packages\\nltk\\__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.downloader import download, download_shell\n",
    "print(nltk.__file__)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39987474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d62c63",
   "metadata": {},
   "source": [
    "### Actual Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17df7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph= \"\"\"Late one night in February 2022, a Chinese man recovering in a Cambodian hospital explained how a group of online scammers had held him against his will, tortured him, and drained his blood. His name is Li Yayuanlun. From his bed, he said he had sought work as a security guard in China, but was tricked and apparently kidnapped, transported to Cambodia, and then sold into one of the country’s many fortified ‘scam compounds’ – facilities where workers labour around the clock to extract money from people online. When he resisted, Li was beaten and tortured, then passed from one scam operator to another. He claimed his captors finally began to drain large quantities of his blood: seven times in half a year. In the hospital, he pointed weakly at an infusion bag above his head. That was the size of each extraction, he said. In online articles, journalists began to describe his case as that of the ‘blood slave’. Li’s tale had all the elements of a nightmare: human trafficking, captivity, physical abuse, and an unexpected macabre twist. In response, Chinese social media lit up with outrage and sympathy. Even government officials in China and Cambodia took notice. For a moment, Li seemed to embody the darkest truths about the criminal scam compounds spreading across parts of Southeast Asia\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b68789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8248d",
   "metadata": {},
   "source": [
    "Apply Stopwords and filter and then apply stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968d79d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['late one night februari 2022 , chines man recov cambodian hospit explain group onlin scammer held , tortur , drain blood .',\n",
       " 'his name li yayuanlun .',\n",
       " 'from bed , said sought work secur guard china , trick appar kidnap , transport cambodia , sold one countri ’ mani fortifi ‘ scam compound ’ – facil worker labour around clock extract money peopl onlin .',\n",
       " 'when resist , li beaten tortur , pass one scam oper anoth .',\n",
       " 'he claim captor final began drain larg quantiti blood : seven time half year .',\n",
       " 'in hospit , point weak infus bag head .',\n",
       " 'that size extract , said .',\n",
       " 'in onlin articl , journalist began describ case ‘ blood slave ’ .',\n",
       " 'li ’ tale element nightmar : human traffick , captiv , physic abus , unexpect macabr twist .',\n",
       " 'in respons , chines social media lit outrag sympathi .',\n",
       " 'even govern offici china cambodia took notic .',\n",
       " 'for moment , li seem embodi darkest truth crimin scam compound spread across part southeast asia']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a45377",
   "metadata": {},
   "source": [
    "Now lets try with Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9bbf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph= \"\"\"Late one night in February 2022, a Chinese man recovering in a Cambodian hospital explained how a group of online scammers had held him against his will, tortured him, and drained his blood. His name is Li Yayuanlun. From his bed, he said he had sought work as a security guard in China, but was tricked and apparently kidnapped, transported to Cambodia, and then sold into one of the country’s many fortified ‘scam compounds’ – facilities where workers labour around the clock to extract money from people online. When he resisted, Li was beaten and tortured, then passed from one scam operator to another. He claimed his captors finally began to drain large quantities of his blood: seven times in half a year. In the hospital, he pointed weakly at an infusion bag above his head. That was the size of each extraction, he said. In online articles, journalists began to describe his case as that of the ‘blood slave’. Li’s tale had all the elements of a nightmare: human trafficking, captivity, physical abuse, and an unexpected macabre twist. In response, Chinese social media lit up with outrage and sympathy. Even government officials in China and Cambodia took notice. For a moment, Li seemed to embody the darkest truths about the criminal scam compounds spreading across parts of Southeast Asia\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4753562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Late one night in February 2022, a Chinese man recovering in a Cambodian hospital explained how a group of online scammers had held him against his will, tortured him, and drained his blood.',\n",
       " 'His name is Li Yayuanlun.',\n",
       " 'From his bed, he said he had sought work as a security guard in China, but was tricked and apparently kidnapped, transported to Cambodia, and then sold into one of the country’s many fortified ‘scam compounds’ – facilities where workers labour around the clock to extract money from people online.',\n",
       " 'When he resisted, Li was beaten and tortured, then passed from one scam operator to another.',\n",
       " 'He claimed his captors finally began to drain large quantities of his blood: seven times in half a year.',\n",
       " 'In the hospital, he pointed weakly at an infusion bag above his head.',\n",
       " 'That was the size of each extraction, he said.',\n",
       " 'In online articles, journalists began to describe his case as that of the ‘blood slave’.',\n",
       " 'Li’s tale had all the elements of a nightmare: human trafficking, captivity, physical abuse, and an unexpected macabre twist.',\n",
       " 'In response, Chinese social media lit up with outrage and sympathy.',\n",
       " 'Even government officials in China and Cambodia took notice.',\n",
       " 'For a moment, Li seemed to embody the darkest truths about the criminal scam compounds spreading across parts of Southeast Asia']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33dd2a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['late one night february 2022 , chinese man recover cambodian hospital explain group online scammers hold , torture , drain blood .',\n",
       " 'his name li yayuanlun .',\n",
       " 'from bed , say seek work security guard china , trick apparently kidnap , transport cambodia , sell one country ’ many fortify ‘ scam compound ’ – facilities workers labour around clock extract money people online .',\n",
       " 'when resist , li beat torture , pass one scam operator another .',\n",
       " 'he claim captors finally begin drain large quantities blood : seven time half year .',\n",
       " 'in hospital , point weakly infusion bag head .',\n",
       " 'that size extraction , say .',\n",
       " 'in online article , journalists begin describe case ‘ blood slave ’ .',\n",
       " 'li ’ tale elements nightmare : human traffic , captivity , physical abuse , unexpected macabre twist .',\n",
       " 'in response , chinese social media light outrage sympathy .',\n",
       " 'even government officials china cambodia take notice .',\n",
       " 'for moment , li seem embody darkest truths criminal scam compound spread across part southeast asia']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower(),pos='v')  for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f14cad",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging\n",
    "We will be finding the pos tag according to words using nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Late', 'RB'), ('one', 'CD'), ('night', 'NN'), ('February', 'NNP'), ('2022', 'CD'), (',', ','), ('Chinese', 'NNP'), ('man', 'NN'), ('recovering', 'VBG'), ('Cambodian', 'JJ'), ('hospital', 'NN'), ('explained', 'VBD'), ('group', 'NN'), ('online', 'NN'), ('scammers', 'NNS'), ('held', 'VBD'), (',', ','), ('tortured', 'VBD'), (',', ','), ('drained', 'VBD'), ('blood', 'NN'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('name', 'NN'), ('Li', 'NNP'), ('Yayuanlun', 'NNP'), ('.', '.')]\n",
      "[('From', 'IN'), ('bed', 'NN'), (',', ','), ('said', 'VBD'), ('sought', 'JJ'), ('work', 'NN'), ('security', 'NN'), ('guard', 'NN'), ('China', 'NNP'), (',', ','), ('tricked', 'VBD'), ('apparently', 'RB'), ('kidnapped', 'VBN'), (',', ','), ('transported', 'JJ'), ('Cambodia', 'NNP'), (',', ','), ('sold', 'VBD'), ('one', 'CD'), ('country', 'NN'), ('’', 'VBD'), ('many', 'JJ'), ('fortified', 'JJ'), ('‘', 'NNP'), ('scam', 'NN'), ('compounds', 'VBZ'), ('’', 'JJ'), ('–', 'NNP'), ('facilities', 'NNS'), ('workers', 'NNS'), ('labour', 'VBP'), ('around', 'IN'), ('clock', 'NN'), ('extract', 'JJ'), ('money', 'NN'), ('people', 'NNS'), ('online', 'VBP'), ('.', '.')]\n",
      "[('When', 'WRB'), ('resisted', 'VBN'), (',', ','), ('Li', 'NNP'), ('beaten', 'RB'), ('tortured', 'VBD'), (',', ','), ('passed', 'VBD'), ('one', 'CD'), ('scam', 'NN'), ('operator', 'NN'), ('another', 'DT'), ('.', '.')]\n",
      "[('He', 'PRP'), ('claimed', 'VBD'), ('captors', 'NNS'), ('finally', 'RB'), ('began', 'VBD'), ('drain', 'JJ'), ('large', 'JJ'), ('quantities', 'NNS'), ('blood', 'NN'), (':', ':'), ('seven', 'CD'), ('times', 'NNS'), ('half', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('In', 'IN'), ('hospital', 'NN'), (',', ','), ('pointed', 'VBD'), ('weakly', 'JJ'), ('infusion', 'NN'), ('bag', 'NN'), ('head', 'NN'), ('.', '.')]\n",
      "[('That', 'DT'), ('size', 'NN'), ('extraction', 'NN'), (',', ','), ('said', 'VBD'), ('.', '.')]\n",
      "[('In', 'IN'), ('online', 'JJ'), ('articles', 'NNS'), (',', ','), ('journalists', 'NNS'), ('began', 'VBD'), ('describe', 'JJ'), ('case', 'NN'), ('‘', 'NNP'), ('blood', 'NN'), ('slave', 'NN'), ('’', 'NNP'), ('.', '.')]\n",
      "[('Li', 'NNP'), ('’', 'NNP'), ('tale', 'JJ'), ('elements', 'NNS'), ('nightmare', 'NN'), (':', ':'), ('human', 'JJ'), ('trafficking', 'NN'), (',', ','), ('captivity', 'NN'), (',', ','), ('physical', 'JJ'), ('abuse', 'NN'), (',', ','), ('unexpected', 'JJ'), ('macabre', 'NN'), ('twist', 'NN'), ('.', '.')]\n",
      "[('In', 'IN'), ('response', 'NN'), (',', ','), ('Chinese', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('lit', 'JJ'), ('outrage', 'NN'), ('sympathy', 'NN'), ('.', '.')]\n",
      "[('Even', 'JJ'), ('government', 'NN'), ('officials', 'NNS'), ('China', 'NNP'), ('Cambodia', 'NNP'), ('took', 'VBD'), ('notice', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('moment', 'NN'), (',', ','), ('Li', 'NNP'), ('seemed', 'VBD'), ('embody', 'JJ'), ('darkest', 'JJ'), ('truths', 'NNS'), ('criminal', 'JJ'), ('scam', 'JJ'), ('compounds', 'NNS'), ('spreading', 'VBG'), ('across', 'IN'), ('parts', 'NNS'), ('Southeast', 'NNP'), ('Asia', 'NNP')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ayush TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Ayush TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph= \"\"\"Late one night in February 2022, a Chinese man recovering in a Cambodian hospital explained how a group of online scammers had held him against his will, tortured him, and drained his blood. His name is Li Yayuanlun. From his bed, he said he had sought work as a security guard in China, but was tricked and apparently kidnapped, transported to Cambodia, and then sold into one of the country’s many fortified ‘scam compounds’ – facilities where workers labour around the clock to extract money from people online. When he resisted, Li was beaten and tortured, then passed from one scam operator to another. He claimed his captors finally began to drain large quantities of his blood: seven times in half a year. In the hospital, he pointed weakly at an infusion bag above his head. That was the size of each extraction, he said. In online articles, journalists began to describe his case as that of the ‘blood slave’. Li’s tale had all the elements of a nightmare: human trafficking, captivity, physical abuse, and an unexpected macabre twist. In response, Chinese social media lit up with outrage and sympathy. Even government officials in China and Cambodia took notice. For a moment, Li seemed to embody the darkest truths about the criminal scam compounds spreading across parts of Southeast Asia\"\"\"\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "import nltk\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "    po_tag = nltk.pos_tag(words)\n",
    "    print(po_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f90b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# for i in \"Taj Mahal is a beautiful Monument\".split():\n",
    "#     print(i)\n",
    "#     print(nltk.pos_tag(i))\n",
    "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e0209",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Tags like Person, Place or Location, Date, Time, Money, Organization, Percent\n",
    "\n",
    "tokenize words --- get the pos_tags list --- use nltk.ne_chunk to get the named entity \n",
    "\n",
    "draw function opens it in tkinter application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f262d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Eiffel tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83637e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Ayush\n",
      "[nltk_data]     TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Ayush TIdke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tagged_elements = nltk.pos_tag(words)\n",
    "nltk.ne_chunk(tagged_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf1f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
